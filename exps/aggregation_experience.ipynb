{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TQ vs (PiT)Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "S, A, K = 1000, 4, 50\n",
    "\n",
    "R = np.random.random(size=(S, A))\n",
    "T = np.random.random(size=(A, S, S))\n",
    "omega = np.random.random(size=(K, S))\n",
    "phi = np.random.random(size=(S, K))\n",
    "T_ = omega @ T @ phi\n",
    "R_ = omega @ R\n",
    "\n",
    "discount = 0.9/S**3\n",
    "\n",
    "q_value = np.zeros((S, A))\n",
    "contracted_q_value = np.zeros((K, A))\n",
    "\n",
    "def bellman_q(q_value: np.ndarray):\n",
    "    return R + (discount * T @ q_value.max(axis=1)).T\n",
    "\n",
    "def pi_bellman_q(contracted_q_value: np.ndarray):\n",
    "    return R_ + (discount * T_ @ contracted_q_value.max(axis=1)).T\n",
    "\n",
    "n_step = 1000\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for _ in range(n_step):\n",
    "    q_value = bellman_q(q_value)\n",
    "\n",
    "print(\"Time exact Bellman : {}\".format(time.time() - start_time))\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for _ in range(n_step):\n",
    "    contracted_q_value = pi_bellman_q(contracted_q_value)\n",
    "\n",
    "print(\"Time projected Bellman : {}\".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### V <-- Pi (R + TV) converge ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from models.rooms_total import Model\n",
    "from utils.calculus import get_full_phi_from_states_in_regions, get_weight_matrix_from_states_in_regions\n",
    "from utils.calculus_projected import create_random_partition\n",
    "from utils.calculus import get_optimal_policy, bellman_no_max\n",
    "\n",
    "model = Model()\n",
    "model.create_model()\n",
    "# model._normalize_reward_matrix()\n",
    "policy = get_optimal_policy(model, 'discounted', .9)\n",
    "model.model_to_numpy()\n",
    "K = int(model.state_dim * 0.9)\n",
    "states_in_regions = create_random_partition(model, K)\n",
    "phi = get_full_phi_from_states_in_regions(model, states_in_regions)[:, :K]\n",
    "omega = get_weight_matrix_from_states_in_regions(model.state_dim, states_in_regions, K)\n",
    "V = np.zeros((model.state_dim))\n",
    "\n",
    "# policy = np.random.randint(low=0, high=model.action_dim, size=model.state_dim)\n",
    "\n",
    "\n",
    "t_pi = np.zeros((model.state_dim, model.state_dim))\n",
    "r_pi = np.zeros((model.state_dim))\n",
    "for state in range(model.state_dim):\n",
    "    action = policy[state]\n",
    "    t_pi[state] = model.transition_matrix[action][state]\n",
    "    r_pi[state] = model.reward_matrix[state, action]\n",
    "\n",
    "value = np.zeros((model.state_dim)) +2 \n",
    "\n",
    "def projected_bellman(value : np.ndarray):\n",
    "    return phi @ omega @ (r_pi + t_pi @ value)\n",
    "\n",
    "for _ in range(1000):\n",
    "    value = projected_bellman(value)\n",
    "    # print(np.linalg.norm(value - projected_bellman(value)))\n",
    "\n",
    "print(np.round(value.reshape((10, 10)), 2))\n",
    "\n",
    "print('Conclusion : Projected Bellman Operator does not converge for the total criterion.')\n",
    "print(\"Conclusion alternative : ça pourrait converger.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### V <-- (min_S_k (R + TV))_k converge ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from models.rooms_total import Model\n",
    "from utils.calculus import get_full_phi_from_states_in_regions, get_weight_matrix_from_states_in_regions\n",
    "from utils.calculus_projected import create_random_partition\n",
    "from utils.calculus import get_optimal_policy, bellman_no_max\n",
    "\n",
    "model = Model()\n",
    "model.create_model()\n",
    "# model._normalize_reward_matrix()\n",
    "policy = get_optimal_policy(model, 'discounted', .9)\n",
    "model.model_to_numpy()\n",
    "K = int(model.state_dim * 0.9)\n",
    "states_in_regions = create_random_partition(model, K)\n",
    "phi = get_full_phi_from_states_in_regions(model, states_in_regions)[:, :K]\n",
    "omega = get_weight_matrix_from_states_in_regions(model.state_dim, states_in_regions, K)\n",
    "V = np.zeros((model.state_dim))\n",
    "\n",
    "# policy = np.random.randint(low=0, high=model.action_dim, size=model.state_dim)\n",
    "\n",
    "\n",
    "t_pi = np.zeros((model.state_dim, model.state_dim))\n",
    "r_pi = np.zeros((model.state_dim))\n",
    "for state in range(model.state_dim):\n",
    "    action = policy[state]\n",
    "    t_pi[state] = model.transition_matrix[action][state]\n",
    "    r_pi[state] = model.reward_matrix[state, action]\n",
    "\n",
    "value = np.zeros((model.state_dim))+2\n",
    "\n",
    "def projected_bellman(value : np.ndarray):\n",
    "    value = r_pi + t_pi @ value\n",
    "    return phi @ np.array([[np.min(value[states_in_regions[k]]) for k in range(K)]]).reshape((K, 1))\n",
    "\n",
    "for _ in range(1000):\n",
    "    value = projected_bellman(value)\n",
    "    # print(np.linalg.norm(value - projected_bellman(value)))\n",
    "\n",
    "# print(np.round(value.reshape((10, 10)), 2))\n",
    "\n",
    "print(\"On a l'impression que oui mais en fait non.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pi T V sparse vs TV sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from models.rooms_total import Model\n",
    "from utils.calculus import get_full_phi_from_states_in_regions, get_weight_matrix_from_states_in_regions\n",
    "from utils.calculus_projected import create_random_partition, projected_optimal_bellman_operator\n",
    "from utils.calculus import get_optimal_policy, bellman_no_max, optimal_bellman_operator\n",
    "from scipy.sparse import csr_matrix\n",
    "import time\n",
    "\n",
    "discount = 0.9\n",
    "epsilon = 1e-2\n",
    "n_step = 100000\n",
    "aggregation_rate = 0.8\n",
    "\n",
    "model = Model()\n",
    "model.create_model()\n",
    "K = int(model.state_dim * aggregation_rate)\n",
    "\n",
    "states_in_regions = create_random_partition(model, K)\n",
    "\n",
    "phi = get_full_phi_from_states_in_regions(model, states_in_regions)[:, :K]\n",
    "omega = get_weight_matrix_from_states_in_regions(model.state_dim, states_in_regions, K)\n",
    "\n",
    "agg_reward = model.reward_matrix\n",
    "\n",
    "agg_transition = [transition.dot(phi) for transition in model.transition_matrix]\n",
    "\n",
    "value = np.zeros((model.state_dim))\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for _ in range(n_step):\n",
    "    value = optimal_bellman_operator(model, value, discount)\n",
    "\n",
    "exact_time = time.time() - start_time\n",
    "\n",
    "print(\"Exact steps time : {}\".format(exact_time))\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "contracted_value = np.zeros((K))\n",
    "\n",
    "for _ in range(n_step):\n",
    "    contracted_value = projected_optimal_bellman_operator(model, discount, contracted_value, agg_transition, agg_reward, omega)\n",
    "\n",
    "agg_time = time.time() - start_time\n",
    "\n",
    "print(\"Agg steps time : {}\".format(agg_time))\n",
    "\n",
    "print(\"Conclusion : les itérations agrégées restent bien meilleures de {} %.\".format(np.round(1 - agg_time / exact_time, 2) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A quoi ressemble la distribution des temps de calcul ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from models.rooms import Model\n",
    "from utils.calculus import get_full_phi_from_states_in_regions, get_weight_matrix_from_states_in_regions\n",
    "from utils.calculus_projected import create_random_partition, projected_optimal_bellman_operator\n",
    "from utils.calculus import get_optimal_policy, bellman_no_max, optimal_bellman_operator\n",
    "from scipy.sparse import csr_matrix\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "discount = 0.9\n",
    "n_sample = 100\n",
    "\n",
    "model = Model()\n",
    "model.create_model()\n",
    "\n",
    "def sample():\n",
    "    value = np.zeros((model.state_dim))\n",
    "    start_time = time.time()\n",
    "    for _ in range(100):\n",
    "        value = optimal_bellman_operator(model, value, discount)\n",
    "\n",
    "    return time.time() - start_time\n",
    "\n",
    "time_sample = [sample() for _ in range(n_sample)]\n",
    "\n",
    "print(np.mean(time_sample))\n",
    "print(np.median(time_sample))\n",
    "\n",
    "plt.hist(time_sample, bins='auto')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exemple de MDP cas total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mdptoolbox.mdp import PolicyIteration\n",
    "import numpy as np\n",
    "\n",
    "T = np.array([[[0.8, 0.2], [1, 0.0]]])\n",
    "R = np.array([[[0, 1], [0, 0]]])\n",
    "vi = PolicyIteration(T, R, 1.)\n",
    "vi.run()\n",
    "vi.V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Faire varier les étapes de Bellman pour la désagrégation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from models.tandem import Model, param_list\n",
    "from solvers.aggregated_vi import Solver\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import trange\n",
    "import time\n",
    "\n",
    "model = Model(param_list[0])\n",
    "model.create_model()\n",
    "model._model_to_numpy()\n",
    "\n",
    "discount = 0.8\n",
    "epsilon = 1e-1\n",
    "bellman_updates = 20\n",
    "\n",
    "updates = trange(bellman_updates)\n",
    "runtimes = []\n",
    "\n",
    "\n",
    "for bellman_updates in updates:\n",
    "\n",
    "    avi = Solver(model, discount, epsilon, False, bellman_updates)\n",
    "    avi.run()\n",
    "    runtimes.append(avi.runtime)\n",
    "\n",
    "print(\"Conclusion : l'hyperparamètre n dépend beaucoup du modèle.\")\n",
    "\n",
    "plt.scatter(list(updates), runtimes)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialisation de Policy Iteration Modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### np.sum vs sum on scipy sparse arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "matrix = csr_matrix((np.random.random((1000, 1000)) < 0.5).astype(int))\n",
    "\n",
    "def timeit(function, n_step = 10000):\n",
    "    start_time = time.time()\n",
    "    for _ in range(n_step):\n",
    "        function()\n",
    "    runtime = (time.time() - start_time)/n_step\n",
    "    return runtime\n",
    "\n",
    "def sum_numpy():\n",
    "    return np.sum(matrix[0])\n",
    "\n",
    "def sum_python():\n",
    "    return sum(matrix[0])\n",
    "\n",
    "def sum_scipy():\n",
    "    return matrix[0].sum()\n",
    "\n",
    "print(timeit(sum_numpy))\n",
    "print(timeit(sum_python))\n",
    "print(timeit(sum_scipy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pour Tandem, a quoi ressemble V^*-R.max() ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.tandem import Model, param_list\n",
    "from utils.exact_value_function import get_exact_value\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.calculus import norminf\n",
    "import numpy as np\n",
    "\n",
    "discount = 0.999\n",
    "eps = 0.04\n",
    "\n",
    "model = Model()\n",
    "model.create_model()\n",
    "optimal_value = get_exact_value(model, 'discounted', discount)\n",
    "value = optimal_value - model.reward_matrix.max(axis=1)\n",
    "\n",
    "def f(x):\n",
    "    return norminf(optimal_value - model.reward_matrix.max(axis=1) * x)\n",
    "\n",
    "values = np.linspace(204, 206, 1000)\n",
    "\n",
    "plt.plot(values, [f(x) for x in values])\n",
    "plt.show()\n",
    "\n",
    "print(values[np.argmin([f(x) for x in values])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot le temps de calcul de VI en fonction du discount, pareil pour PIM, PDVI, PDQVI, PDPIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from solvers.aggregated_vi import Solver as Avi\n",
    "from solvers.aggregated_qvi import Solver as Aqvi\n",
    "from solvers.aggregated_pim import Solver as Apim\n",
    "\n",
    "from solvers.personal_vi import Solver as Vi\n",
    "from solvers.personal_pim import Solver as Pim\n",
    "from solvers.bertsekas_pi import Solver as Bert\n",
    "from solvers.chen_td import Solver as Chen\n",
    "\n",
    "from utils.generic_model import NUMPY, SPARSE\n",
    "\n",
    "import numpy as np\n",
    "from models.rooms import Model, param_list\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import trange, tqdm\n",
    "\n",
    "level = 1\n",
    "discounts = [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 0.8, 0.85, 0.9, 0.95, 0.99, 0.995, 0.999, 0.9995, 0.9999, 0.99995]\n",
    "f = lambda x : np.log10(x) - np.log10(1-x)\n",
    "discounts_plot = [f(discount) for discount in discounts]\n",
    "epsilon = 1e-1\n",
    "env = Model(param_list[level])\n",
    "env.create_model(mode=SPARSE, check_transition=False)\n",
    "\n",
    "print(env.state_dim)\n",
    "\n",
    "solvers = [Avi, Aqvi, Apim, Vi, Pim]\n",
    "results = {}\n",
    "\n",
    "for discount_index in trange(len(discounts)):\n",
    "    discount = discounts[discount_index]\n",
    "    for Solver in solvers:\n",
    "        solver = Solver(env, discount, epsilon)\n",
    "        solver.run()\n",
    "        try:\n",
    "            results[solver.name].append(solver.runtime)\n",
    "        except KeyError:\n",
    "            results[solver.name] = [solver.runtime]\n",
    "\n",
    "plt.yscale('log')\n",
    "\n",
    "for (solver_name, runtimes) in results.items():\n",
    "    plt.plot(discounts_plot, runtimes, label=solver_name)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from solvers.aggregated_vi import Solver as Avi\n",
    "from solvers.aggregated_qvi import Solver as Aqvi\n",
    "from solvers.aggregated_pim import Solver as Apim\n",
    "\n",
    "from solvers.personal_vi import Solver as Vi\n",
    "from solvers.personal_pim import Solver as Pim\n",
    "from solvers.bertsekas_pi import Solver as Bert\n",
    "from solvers.chen_td import Solver as Chen\n",
    "\n",
    "from utils.generic_model import NUMPY, SPARSE\n",
    "\n",
    "import numpy as np\n",
    "from models.rooms import Model, param_list\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import trange, tqdm\n",
    "\n",
    "level = 2\n",
    "discounts = [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 0.8, 0.85, 0.9, 0.95, 0.99, 0.995, 0.999, 0.9995, 0.9999, 0.99995]\n",
    "f = lambda x : np.log(x) - np.log(1-x)\n",
    "discounts_plot = [f(discount) for discount in discounts]\n",
    "epsilon = 1e-1\n",
    "env = Model(param_list[level])\n",
    "env.create_model(mode=SPARSE, check_transition=False)\n",
    "\n",
    "print(env.state_dim)\n",
    "\n",
    "solvers = [Avi, Aqvi, Apim, Vi, Pim]\n",
    "results = {}\n",
    "\n",
    "for discount in tqdm(discounts):\n",
    "    for Solver in solvers:\n",
    "        solver = Solver(env, discount, epsilon)\n",
    "        solver.run()\n",
    "        try:\n",
    "            results[solver.name].append(solver.runtime)\n",
    "        except KeyError:\n",
    "            results[solver.name] = [solver.runtime]\n",
    "\n",
    "plt.yscale('log')\n",
    "\n",
    "for (solver_name, runtimes) in results.items():\n",
    "    plt.plot(discounts_plot, runtimes, label=solver_name)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $Span V* <= \\epsilon$ and $\\tilde{V}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.generic_model import NUMPY, SPARSE\n",
    "from models.tandem import Model, param_list\n",
    "from utils.exact_value_function import get_exact_value, distance_to_optimal\n",
    "from utils.partition_value import ValuePartition\n",
    "from utils.calculus_projected import apply_pobo_until_var_small, projected_optimal_bellman_operator\n",
    "from utils.calculus import norminf, get_value_policy_value\n",
    "\n",
    "discount = 0.99\n",
    "epsilon = 0.2\n",
    "level = 0\n",
    "model = Model(param_list[level])\n",
    "model.create_model(mode=SPARSE, check_transition=False)\n",
    "exact_value = get_exact_value(model, 'discounted', discount)\n",
    "\n",
    "contracted_value = np.zeros((1))\n",
    "\n",
    "partition = ValuePartition(model, discount, SPARSE)\n",
    "contracted_value = partition.divide_regions_along_tv(exact_value, epsilon, contracted_value)\n",
    "partition._build_full_phi()\n",
    "partition._build_weights(True)\n",
    "agg_tr, agg_r = partition._compute_aggregate_transition_reward()\n",
    "\n",
    "contracted_value, _ = apply_pobo_until_var_small(model, discount, agg_tr, agg_r, partition.weights, 1e-3)\n",
    "v_tilde = partition._partial_phi() @ contracted_value\n",
    "\n",
    "print(distance_to_optimal(v_tilde, model, 'discounted', discount))\n",
    "\n",
    "def compute_precision(epsilon):\n",
    "    global model\n",
    "    global discount\n",
    "\n",
    "    contracted_value = np.zeros((1))\n",
    "\n",
    "    partition = ValuePartition(model, discount, SPARSE)\n",
    "    contracted_value = partition.divide_regions_along_tv(exact_value, epsilon, contracted_value)\n",
    "    partition._build_full_phi()\n",
    "    partition._build_weights(True)\n",
    "    agg_tr, agg_r = partition._compute_aggregate_transition_reward()\n",
    "\n",
    "    contracted_value, _ = apply_pobo_until_var_small(model, discount, agg_tr, agg_r, partition.weights, 1e-1, contracted_value, SPARSE)\n",
    "\n",
    "\n",
    "    v_tilde = np.array(partition._partial_phi() @ contracted_value)\n",
    "    error_to_optimal = distance_to_optimal(v_tilde, model, 'discounted', discount)\n",
    "    partition_shrinkage = partition._number_of_regions()/model.state_dim\n",
    "    v_pi_v = get_value_policy_value(model, discount, v_tilde, 1e-1, SPARSE)\n",
    "    v_pi_v_v = distance_to_optimal(v_pi_v, model, 'discounted', discount)\n",
    "\n",
    "    results = {\n",
    "        'distance_to_optimal' : error_to_optimal,\n",
    "        'v_tilde' : v_tilde,\n",
    "        'partition_shrinkage' : partition_shrinkage,\n",
    "        'v_pi_v_v' : v_pi_v_v,\n",
    "    }\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = 10**np.linspace(-6, 3, 15)\n",
    "\n",
    "experiment = [compute_precision(eps) for eps in tqdm(X)]\n",
    "v_v = [result['distance_to_optimal'] for result in experiment]\n",
    "v_pi = [result['v_pi_v_v'] for result in experiment]\n",
    "\n",
    "plt.plot(X, v_v/X, label='|Vt - V^*|')\n",
    "plt.plot(X, v_pi/X, label='|Vpi - V^*|')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.generic_model import NUMPY, SPARSE\n",
    "from models.rooms import Model, param_list\n",
    "from utils.exact_value_function import get_exact_value, distance_to_optimal\n",
    "from utils.partition_value import ValuePartition\n",
    "from utils.calculus_projected import apply_pobo_until_var_small, projected_optimal_bellman_operator\n",
    "from utils.calculus import norminf, get_value_policy_value\n",
    "from utils.partition_generic import GenericPartition\n",
    "\n",
    "discount = 0.99\n",
    "epsilon = 1e-2\n",
    "level = 0\n",
    "model = Model(param_list[level])\n",
    "model.create_model(mode=SPARSE, check_transition=False)\n",
    "exact_value = get_exact_value(model, 'discounted', discount)\n",
    "\n",
    "contracted_value = np.zeros((4))\n",
    "\n",
    "partition = GenericPartition(model, discount, 'sparse')\n",
    "region1 = []\n",
    "for i in range(5):\n",
    "    region1+=list(range(10*i, 10*i+5))\n",
    "region2 = []\n",
    "for i in range(5):\n",
    "    region2+=list(range(5+10*i, 5+10*i+5))\n",
    "region3 = []\n",
    "for i in range(5):\n",
    "    region3+=list(range(50+10*i, 50+10*i+5))\n",
    "region4 = []\n",
    "for i in range(5):\n",
    "    region4+=list(range(55+10*i, 55+10*i+5))\n",
    "partition.states_in_region = [region1, region2, region3, region4]\n",
    "partition._compute_weights_phi()\n",
    "partition.compute_agg_trans_reward_v();\n",
    "\n",
    "import seaborn as sns\n",
    "contracted_value = np.zeros((4))\n",
    "\n",
    "\n",
    "for i in range(3):\n",
    "    value = partition._partial_phi() @ contracted_value\n",
    "    value = value.reshape((10, 10))\n",
    "    sns.heatmap(value)\n",
    "    plt.savefig('images/rooms_partition_{}'.format(i))\n",
    "    plt.show()\n",
    "    for _ in range(12):\n",
    "        contracted_value = projected_optimal_bellman_operator(model, discount, contracted_value, partition.aggregate_transition_matrix, partition.aggregate_reward_matrix, partition.weights)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.generic_model import NUMPY, SPARSE\n",
    "from models.rooms import Model, param_list\n",
    "from utils.exact_value_function import get_exact_value, distance_to_optimal\n",
    "from utils.partition_value import ValuePartition\n",
    "from utils.calculus_projected import apply_pobo_until_var_small, projected_optimal_bellman_operator\n",
    "from utils.calculus import norminf, get_value_policy_value, optimal_bellman_operator\n",
    "from utils.partition_generic import GenericPartition\n",
    "\n",
    "discount = 0.99\n",
    "epsilon = 1e-2\n",
    "level = 0\n",
    "model = Model(param_list[level])\n",
    "model.create_model(mode=SPARSE, check_transition=False)\n",
    "exact_value = get_exact_value(model, 'discounted', discount)\n",
    "\n",
    "contracted_value = np.zeros((1))\n",
    "partition = GenericPartition(model, discount, 'sparse')\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "partition._compute_weights_phi()\n",
    "partition.compute_agg_trans_reward_v()\n",
    "\n",
    "for i in range(4):\n",
    "    for _ in range(100):\n",
    "        contracted_value = projected_optimal_bellman_operator(model, discount, contracted_value, partition.aggregate_transition_matrix, partition.aggregate_reward_matrix, partition.weights)\n",
    "\n",
    "    value = partition._partial_phi() @ contracted_value\n",
    "    bellman_value = optimal_bellman_operator(model, value, discount)\n",
    "    contracted_value = partition.divide_all_regions_along_value_update_contracted_value(bellman_value, 0.01, contracted_value)\n",
    "    partition._compute_weights_phi()\n",
    "    partition.compute_agg_trans_reward_v()\n",
    "\n",
    "    value = partition._partial_phi() @ contracted_value\n",
    "    value = value.reshape((10, 10))\n",
    "    sns.heatmap(value)\n",
    "    plt.savefig('images/rooms_pdvi_{}'.format(i))\n",
    "    plt.show()\n",
    "\n",
    "for i in range(200):\n",
    "    for _ in range(100):\n",
    "        contracted_value = projected_optimal_bellman_operator(model, discount, contracted_value, partition.aggregate_transition_matrix, partition.aggregate_reward_matrix, partition.weights)\n",
    "\n",
    "    value = partition._partial_phi() @ contracted_value\n",
    "    bellman_value = optimal_bellman_operator(model, value, discount)\n",
    "    contracted_value = partition.divide_all_regions_along_value_update_contracted_value(bellman_value, 0.01, contracted_value)\n",
    "    partition._compute_weights_phi()\n",
    "    partition.compute_agg_trans_reward_v()\n",
    "\n",
    "value = partition._partial_phi() @ contracted_value\n",
    "value = value.reshape((10, 10))\n",
    "sns.heatmap(value)\n",
    "plt.savefig('images/rooms_pdvi_4')\n",
    "plt.show()\n",
    "\n",
    "print(partition._number_of_regions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Il se passe quoi si on dépasse V^* dans le cas total ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.generic_model import NUMPY, SPARSE\n",
    "from models.rooms import Model, param_list\n",
    "from utils.exact_value_function import get_exact_value, distance_to_optimal\n",
    "from utils.partition_value import ValuePartition\n",
    "from utils.calculus_projected import apply_pobo_until_var_small, projected_optimal_bellman_operator\n",
    "from utils.calculus import norminf, get_value_policy_value, optimal_bellman_operator\n",
    "from utils.partition_generic import GenericPartition\n",
    "import seaborn as sns\n",
    "\n",
    "discount = 1.\n",
    "epsilon = 1e-2\n",
    "level = 0\n",
    "model = Model(param_list[level])\n",
    "model.create_model(mode=SPARSE, check_transition=False, normalize_reward=False)\n",
    "\n",
    "value = 0 * np.ones((model.state_dim))\n",
    "\n",
    "for iter in range(80):\n",
    "    value = optimal_bellman_operator(model, value, discount)\n",
    "    if np.random.random() < 0.3:\n",
    "        plt.plot(value, label='iter {}'.format(iter))\n",
    "\n",
    "plt.plot(value-1)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "value = -100 * np.ones((model.state_dim))\n",
    "\n",
    "for iter in range(100):\n",
    "    value = optimal_bellman_operator(model, value, discount)\n",
    "    if np.random.random() < 0.1:\n",
    "        plt.plot(value, label='iter {}'.format(iter))\n",
    "\n",
    "plt.plot(value-1)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PDQVI cas total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.generic_model import NUMPY, SPARSE\n",
    "from models.rooms import Model, param_list\n",
    "from utils.exact_value_function import get_exact_value, distance_to_optimal\n",
    "from utils.partition_value import ValuePartition\n",
    "from utils.partition_q_value import QValuePartition\n",
    "from utils.calculus_projected import apply_pobo_until_var_small, projected_optimal_bellman_operator, projected_optimal_q_bellman_operator\n",
    "from utils.calculus import norminf, get_value_policy_value, optimal_bellman_operator, q_optimal_bellman_operator\n",
    "from utils.partition_generic import GenericPartition\n",
    "import seaborn as sns\n",
    "\n",
    "discount = 1.\n",
    "epsilon = 1e-2\n",
    "level = 0\n",
    "model = Model(param_list[level])\n",
    "model.create_model(mode=SPARSE, check_transition=False, normalize_reward=False)\n",
    "\n",
    "partition = QValuePartition(model, discount, SPARSE)\n",
    "contracted_q_value = 0 * np.ones((partition._number_of_regions(), model.action_dim))\n",
    "\n",
    "partition._compute_aggregate_transition_reward()\n",
    "\n",
    "for _ in range(20):\n",
    "    partition._compute_weights_phi()\n",
    "    partition.compute_agg_trans_reward_q()\n",
    "\n",
    "    for _ in range(100):\n",
    "        prev = contracted_q_value\n",
    "        contracted_q_value = projected_optimal_q_bellman_operator(model, discount, prev, partition.aggregate_transition_matrix, partition.aggregate_reward_matrix)\n",
    "    \n",
    "    q_value = partition._partial_phi().dot(contracted_q_value)\n",
    "    sns.heatmap(q_value.max(axis=1).reshape((10, 10)))\n",
    "    plt.show()\n",
    "\n",
    "    bellman = q_optimal_bellman_operator(model, q_value, discount)\n",
    "\n",
    "    for action in range(model.action_dim):\n",
    "        contracted_q_value = partition.divide_all_regions_along_value_update_contracted_q_value(bellman[:, action], epsilon, contracted_q_value)\n",
    "    # print(partition._number_of_regions())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.generic_model import NUMPY, SPARSE\n",
    "from models.rooms import Model, param_list\n",
    "from utils.exact_value_function import get_exact_value, distance_to_optimal\n",
    "from utils.partition_value import ValuePartition\n",
    "from utils.partition_q_value import QValuePartition\n",
    "from utils.calculus_projected import apply_pobo_until_var_small, projected_optimal_bellman_operator, projected_optimal_q_bellman_operator\n",
    "from utils.calculus import norminf, get_value_policy_value, optimal_bellman_operator, q_optimal_bellman_operator\n",
    "from utils.partition_generic import GenericPartition\n",
    "import seaborn as sns\n",
    "\n",
    "discount = 1.\n",
    "epsilon = 1e-2\n",
    "level = 0\n",
    "model = Model(param_list[level])\n",
    "model.create_model(mode=SPARSE, check_transition=False, normalize_reward=False)\n",
    "\n",
    "s1, s2 = 25, 75\n",
    "\n",
    "for aa in range(model.action_dim):\n",
    "    model.transition_matrix[aa][s1, :] = 0.\n",
    "    model.transition_matrix[aa][s1, s1] = 1.\n",
    "\n",
    "    model.transition_matrix[aa][s2, :] = 0.\n",
    "    model.transition_matrix[aa][s2, s2] = 1.\n",
    "\n",
    "for k in range(10):\n",
    "    for l in range(5, 10):\n",
    "        index = k*10+l\n",
    "        model.reward_matrix[index, :] = 0.\n",
    "\n",
    "model.reward_matrix[99, :] = -1.\n",
    "\n",
    "partition = QValuePartition(model, discount, SPARSE)\n",
    "contracted_q_value = 0 * np.ones((partition._number_of_regions(), model.action_dim))\n",
    "\n",
    "partition._compute_aggregate_transition_reward()\n",
    "\n",
    "for _ in range(20):\n",
    "    partition._compute_weights_phi()\n",
    "    partition.compute_agg_trans_reward_q()\n",
    "\n",
    "    for _ in range(100):\n",
    "        prev = contracted_q_value\n",
    "        contracted_q_value = projected_optimal_q_bellman_operator(model, discount, prev, partition.aggregate_transition_matrix, partition.aggregate_reward_matrix)\n",
    "    \n",
    "    q_value = partition._partial_phi().dot(contracted_q_value)\n",
    "    sns.heatmap(q_value.max(axis=1).reshape((10, 10)))\n",
    "    plt.show()\n",
    "\n",
    "    bellman = q_optimal_bellman_operator(model, q_value, discount)\n",
    "\n",
    "    for action in range(model.action_dim):\n",
    "        contracted_q_value = partition.divide_all_regions_along_value_update_contracted_q_value(bellman[:, action], epsilon, contracted_q_value)\n",
    "    # print(partition._number_of_regions())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.generic_model import NUMPY, SPARSE\n",
    "from models.rooms import Model, param_list\n",
    "from utils.exact_value_function import get_exact_value, distance_to_optimal\n",
    "from utils.partition_value import ValuePartition\n",
    "from utils.partition_q_value import QValuePartition\n",
    "from utils.calculus_projected import apply_pobo_until_var_small, projected_optimal_bellman_operator, projected_optimal_q_bellman_operator\n",
    "from utils.calculus import norminf, get_value_policy_value, optimal_bellman_operator, q_optimal_bellman_operator\n",
    "from utils.partition_generic import GenericPartition\n",
    "import seaborn as sns\n",
    "\n",
    "discount = 1.\n",
    "epsilon = 1e-2\n",
    "level = 0\n",
    "model = Model(param_list[level])\n",
    "model.create_model(mode=SPARSE, check_transition=False, normalize_reward=False)\n",
    "\n",
    "model.transition_matrix[0][12, 2] = 1.\n",
    "model.transition_matrix[0][12, 12] = 0.\n",
    "\n",
    "partition = QValuePartition(model, discount, SPARSE)\n",
    "\n",
    "contracted_q_value = -15 * np.ones((partition._number_of_regions(), model.action_dim))\n",
    "\n",
    "partition._compute_aggregate_transition_reward()\n",
    "\n",
    "for _ in range(20):\n",
    "    partition._compute_weights_phi()\n",
    "    partition.compute_agg_trans_reward_q()\n",
    "\n",
    "    for _ in range(100):\n",
    "        prev = contracted_q_value\n",
    "        contracted_q_value = projected_optimal_q_bellman_operator(model, discount, prev, partition.aggregate_transition_matrix, partition.aggregate_reward_matrix)\n",
    "    \n",
    "    q_value = partition._partial_phi().dot(contracted_q_value)\n",
    "    sns.heatmap(q_value.max(axis=1).reshape((10, 10)))\n",
    "    plt.show()\n",
    "\n",
    "    bellman = q_optimal_bellman_operator(model, q_value, discount)\n",
    "\n",
    "    for action in range(model.action_dim):\n",
    "        contracted_q_value = partition.divide_all_regions_along_value_update_contracted_q_value(bellman[:, action], epsilon, contracted_q_value)\n",
    "    # print(partition._number_of_regions())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.generic_model import NUMPY, SPARSE\n",
    "from models.rooms import Model, param_list\n",
    "from utils.exact_value_function import get_exact_value, distance_to_optimal\n",
    "from utils.partition_value import ValuePartition\n",
    "from utils.partition_q_value import QValuePartition\n",
    "from utils.calculus_projected import apply_pobo_until_var_small, projected_optimal_bellman_operator, projected_optimal_q_bellman_operator\n",
    "from utils.calculus import norminf, get_value_policy_value, optimal_bellman_operator, q_optimal_bellman_operator\n",
    "from utils.partition_generic import GenericPartition\n",
    "import seaborn as sns\n",
    "\n",
    "discount = 1.\n",
    "epsilon = 1e-2\n",
    "level = 0\n",
    "model = Model(param_list[level])\n",
    "model.create_model(mode=SPARSE, check_transition=False, normalize_reward=False)\n",
    "\n",
    "model.reward_matrix -= 1\n",
    "\n",
    "value = np.zeros((model.state_dim))\n",
    "\n",
    "for _ in range(4):\n",
    "    value = optimal_bellman_operator(model, value, discount)\n",
    "    plt.plot(value)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.generic_model import NUMPY, SPARSE\n",
    "from models.rooms import Model, param_list\n",
    "from utils.exact_value_function import get_exact_value, distance_to_optimal\n",
    "from utils.partition_value import ValuePartition\n",
    "from utils.partition_q_value import QValuePartition\n",
    "from utils.calculus_projected import apply_pobo_until_var_small, projected_optimal_bellman_operator, projected_optimal_q_bellman_operator\n",
    "from utils.calculus import norminf, get_value_policy_value, optimal_bellman_operator, q_optimal_bellman_operator\n",
    "from utils.partition_generic import GenericPartition\n",
    "import seaborn as sns\n",
    "\n",
    "discount = 1.\n",
    "epsilon = 1e-2\n",
    "level = 0\n",
    "model = Model(param_list[level])\n",
    "model.create_model(mode=SPARSE, check_transition=False, normalize_reward=False)\n",
    "\n",
    "# model.reward_matrix -= 1\n",
    "\n",
    "value = np.zeros((model.state_dim))-100\n",
    "\n",
    "for _ in range(10):\n",
    "    value = optimal_bellman_operator(model, value, discount)\n",
    "    plt.plot(value)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Différents weights pour partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from utils.generic_model import NUMPY, SPARSE\n",
    "from utils.exact_value_function import get_exact_value, distance_to_optimal\n",
    "from utils.partition_value import ValuePartition\n",
    "from utils.partition_q_value import QValuePartition\n",
    "from utils.calculus_projected import apply_pobo_until_var_small, projected_optimal_bellman_operator, projected_optimal_q_bellman_operator\n",
    "from utils.calculus import norminf, get_value_policy_value, optimal_bellman_operator, q_optimal_bellman_operator\n",
    "from utils.partition_generic import GenericPartition\n",
    "import seaborn as sns\n",
    "from models.rooms import Model, param_list\n",
    "\n",
    "discount = 0.999\n",
    "epsilon = 1e-2\n",
    "level = 1\n",
    "model = Model(param_list[level])\n",
    "model.create_model(mode=SPARSE, check_transition=False, normalize_reward=True)\n",
    "\n",
    "from solvers.aggregated_vi import Solver\n",
    "\n",
    "solver = Solver(model, discount, epsilon)\n",
    "solver.run(False)\n",
    "print(solver.runtime)\n",
    "print(distance_to_optimal(solver.value, model, 'discounted', discount))\n",
    "\n",
    "solver = Solver(model, discount, epsilon)\n",
    "solver.run(True)\n",
    "print(solver.runtime)\n",
    "print(distance_to_optimal(solver.value, model, 'discounted', discount))\n",
    "\n",
    "solver = Solver(model, discount, epsilon)\n",
    "solver.run(True, 'average_reward')\n",
    "print(solver.runtime)\n",
    "print(distance_to_optimal(solver.value, model, 'discounted', discount))\n",
    "\n",
    "solver = Solver(model, discount, epsilon)\n",
    "solver.run(True, 'average_average_reward')\n",
    "print(solver.runtime)\n",
    "print(distance_to_optimal(solver.value, model, 'discounted', discount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from utils.generic_model import NUMPY, SPARSE\n",
    "from utils.exact_value_function import get_exact_value, distance_to_optimal\n",
    "from utils.partition_value import ValuePartition\n",
    "from utils.partition_q_value import QValuePartition\n",
    "from utils.calculus_projected import apply_pobo_until_var_small, projected_optimal_bellman_operator, projected_optimal_q_bellman_operator\n",
    "from utils.calculus import norminf, get_value_policy_value, optimal_bellman_operator, q_optimal_bellman_operator\n",
    "from utils.partition_generic import GenericPartition\n",
    "import seaborn as sns\n",
    "from models.dam import Model, param_list\n",
    "\n",
    "discount = 0.999\n",
    "epsilon = 1e-2\n",
    "level = 0\n",
    "model = Model(param_list[level])\n",
    "model.create_model(mode=SPARSE, check_transition=False, normalize_reward=True)\n",
    "\n",
    "from solvers.aggregated_vi import Solver\n",
    "\n",
    "solver = Solver(model, discount, epsilon)\n",
    "solver.run(False)\n",
    "print(solver.runtime)\n",
    "print(distance_to_optimal(solver.value, model, 'discounted', discount))\n",
    "\n",
    "solver = Solver(model, discount, epsilon)\n",
    "solver.run(True)\n",
    "print(solver.runtime)\n",
    "print(distance_to_optimal(solver.value, model, 'discounted', discount))\n",
    "\n",
    "solver = Solver(model, discount, epsilon)\n",
    "solver.run(True, 'average_reward')\n",
    "print(solver.runtime)\n",
    "print(distance_to_optimal(solver.value, model, 'discounted', discount))\n",
    "\n",
    "solver = Solver(model, discount, epsilon)\n",
    "solver.run(True, 'average_average_reward')\n",
    "print(solver.runtime)\n",
    "print(distance_to_optimal(solver.value, model, 'discounted', discount))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Autres méthodes de partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from utils.generic_model import NUMPY, SPARSE\n",
    "from utils.exact_value_function import get_exact_value, distance_to_optimal\n",
    "from utils.partition_value import ValuePartition\n",
    "from utils.partition_q_value import QValuePartition\n",
    "from utils.calculus_projected import apply_pobo_until_var_small, projected_optimal_bellman_operator, projected_optimal_q_bellman_operator\n",
    "from utils.calculus import norminf, get_value_policy_value, optimal_bellman_operator, q_optimal_bellman_operator\n",
    "from utils.partition_generic import GenericPartition\n",
    "import seaborn as sns\n",
    "from models.rooms import Model, param_list\n",
    "\n",
    "discount = 0.999\n",
    "epsilon = 1e-2\n",
    "level = 0\n",
    "model = Model(param_list[level])\n",
    "model.create_model(mode=SPARSE, check_transition=False, normalize_reward=True)\n",
    "\n",
    "from solvers.aggregated_vi import Solver\n",
    "\n",
    "solver = Solver(model, discount, epsilon)\n",
    "solver.run(False)\n",
    "print(solver.runtime)\n",
    "print(distance_to_optimal(solver.value, model, 'discounted', discount))\n",
    "\n",
    "solver = Solver(model, discount, epsilon)\n",
    "solver.run(True)\n",
    "print(solver.runtime)\n",
    "print(distance_to_optimal(solver.value, model, 'discounted', discount))\n",
    "\n",
    "solver = Solver(model, discount, epsilon)\n",
    "solver.run(True, 'average_reward')\n",
    "print(solver.runtime)\n",
    "print(distance_to_optimal(solver.value, model, 'discounted', discount))\n",
    "\n",
    "solver = Solver(model, discount, epsilon)\n",
    "solver.run(True, 'average_average_reward')\n",
    "print(solver.runtime)\n",
    "print(distance_to_optimal(solver.value, model, 'discounted', discount))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PC Policy Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from utils.generic_model import NUMPY, SPARSE\n",
    "from utils.exact_value_function import get_exact_value, distance_to_optimal\n",
    "from utils.partition_value import ValuePartition\n",
    "from utils.partition_q_value import QValuePartition\n",
    "from utils.calculus_projected import apply_pobo_until_var_small, projected_optimal_bellman_operator, projected_optimal_q_bellman_operator\n",
    "from utils.calculus import norminf, get_value_policy_value, optimal_bellman_operator, q_optimal_bellman_operator, iterative_policy_evaluation, compute_transition_reward_policy, full_iterative_policy_evaluation\n",
    "from utils.partition_generic import GenericPartition\n",
    "import seaborn as sns\n",
    "from models.tandem import Model, param_list\n",
    "from tqdm import trange\n",
    "\n",
    "discount = 0.9\n",
    "epsilon = 1e-2\n",
    "level = 0\n",
    "model = Model(param_list[level])\n",
    "model.create_model(mode=NUMPY, check_transition=False, normalize_reward=True)\n",
    "model._model_to_numpy()\n",
    "\n",
    "\n",
    "value = np.random.random(size=(model.state_dim))\n",
    "for _ in range(1000):\n",
    "    value = optimal_bellman_operator(model, value, discount)\n",
    "\n",
    "res = []\n",
    "\n",
    "for rn in trange(1, 440, 20):\n",
    "\n",
    "    partition = GenericPartition(model, discount, SPARSE)\n",
    "    partition.generate_random_partition(rn)\n",
    "    partition._compute_weights_phi()\n",
    "\n",
    "    q_value = model.reward_matrix + discount * (model.transition_matrix @ value).T\n",
    "    contracted_q_value = partition.weights @ q_value\n",
    "    contracted_policy = partition._partial_phi() @ contracted_q_value.argmax(axis=1)\n",
    "    policy = q_value.argmax(axis=1)\n",
    "    p_value = full_iterative_policy_evaluation(model, discount, policy, 1e-2, NUMPY)\n",
    "    cp_value = full_iterative_policy_evaluation(model, discount, contracted_policy, 1e-2, NUMPY)\n",
    "\n",
    "    res.append(np.abs(p_value-cp_value).mean()*(1-discount))\n",
    "\n",
    "plt.plot(res)\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pi^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from utils.generic_model import NUMPY, SPARSE\n",
    "from utils.exact_value_function import get_exact_value, distance_to_optimal\n",
    "from utils.partition_value import ValuePartition\n",
    "from utils.partition_q_value import QValuePartition\n",
    "from utils.calculus_projected import apply_pobo_until_var_small, projected_optimal_bellman_operator, projected_optimal_q_bellman_operator\n",
    "from utils.calculus import norminf, get_value_policy_value, optimal_bellman_operator, q_optimal_bellman_operator, iterative_policy_evaluation, compute_transition_reward_policy, full_iterative_policy_evaluation, bellman_no_max\n",
    "from utils.partition_generic import GenericPartition\n",
    "import seaborn as sns\n",
    "from models.rooms import Model, param_list\n",
    "from tqdm import trange\n",
    "\n",
    "discount = 0.999\n",
    "epsilon = 1e-2\n",
    "level = 0\n",
    "model = Model(param_list[level])\n",
    "model.create_model(mode=NUMPY, check_transition=False, normalize_reward=False)\n",
    "model._model_to_numpy()\n",
    "\n",
    "\n",
    "value = np.random.random(size=(model.state_dim))\n",
    "for _ in range(1000):\n",
    "    value = optimal_bellman_operator(model, value, discount)\n",
    "\n",
    "policy = np.random.randint(model.action_dim, size=model.state_dim)\n",
    "policy = np.argmax(bellman_no_max(model, value, discount), axis=1)\n",
    "value_p = full_iterative_policy_evaluation(model, discount, policy, 1e-3)\n",
    "\n",
    "tp, rp = compute_transition_reward_policy(model, policy)\n",
    "tp2 = tp @ tp\n",
    "rp2 = rp + discount * tp * rp\n",
    "\n",
    "iterative_policy_evaluation(tp2, rp2, discount, 1e-3, NUMPY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TV exact et Pi T V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from utils.generic_model import NUMPY, SPARSE\n",
    "from utils.exact_value_function import get_exact_value, distance_to_optimal\n",
    "from utils.partition_value import ValuePartition\n",
    "from utils.partition_q_value import QValuePartition\n",
    "from utils.calculus_projected import apply_pobo_until_var_small, projected_optimal_bellman_operator, projected_optimal_q_bellman_operator\n",
    "from utils.calculus import norminf, get_value_policy_value, optimal_bellman_operator, q_optimal_bellman_operator, iterative_policy_evaluation, compute_transition_reward_policy, full_iterative_policy_evaluation, bellman_no_max\n",
    "from utils.partition_generic import GenericPartition\n",
    "import seaborn as sns\n",
    "from models.rooms import Model\n",
    "from tqdm import trange\n",
    "\n",
    "discount = 0.999\n",
    "epsilon = 1e-2\n",
    "level = 0\n",
    "model = Model(100, 4)\n",
    "model.create_model(mode=NUMPY, check_transition=False, normalize_reward=False)\n",
    "model._model_to_numpy()\n",
    "\n",
    "value = np.zeros((model.state_dim))\n",
    "\n",
    "results = []\n",
    "\n",
    "n = 20\n",
    "\n",
    "for i in range(n):\n",
    "    value = optimal_bellman_operator(model, value, discount)\n",
    "    results.append(len(set((value))))\n",
    "\n",
    "plt.plot(results)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from utils.generic_model import NUMPY, SPARSE\n",
    "from utils.exact_value_function import get_exact_value, distance_to_optimal\n",
    "from utils.partition_value import ValuePartition\n",
    "from utils.partition_q_value import QValuePartition\n",
    "from utils.calculus_projected import apply_pobo_until_var_small, projected_optimal_bellman_operator, projected_optimal_q_bellman_operator\n",
    "from utils.calculus import norminf, get_value_policy_value, optimal_bellman_operator, q_optimal_bellman_operator, iterative_policy_evaluation, compute_transition_reward_policy, full_iterative_policy_evaluation, bellman_no_max\n",
    "from utils.partition_generic import GenericPartition\n",
    "import seaborn as sns\n",
    "from models.rooms import Model, param_list\n",
    "from tqdm import trange\n",
    "\n",
    "mode = SPARSE\n",
    "\n",
    "\n",
    "discount = 0.9\n",
    "epsilon = 1e-2\n",
    "level = 0\n",
    "model = Model(param_list[level])\n",
    "model.create_model(mode=NUMPY, check_transition=False, normalize_reward=False)\n",
    "model._model_to_numpy()\n",
    "\n",
    "partition = GenericPartition(model, discount, mode)\n",
    "\n",
    "value = np.zeros((model.state_dim))\n",
    "\n",
    "results = []\n",
    "\n",
    "n = 20\n",
    "\n",
    "for i in range(n):\n",
    "    value = optimal_bellman_operator(model, value, discount)\n",
    "    results.append(len(set((value))))\n",
    "\n",
    "plt.plot(results)\n",
    "\n",
    "discount = 0.999\n",
    "epsilon = 1e-2\n",
    "level = 0\n",
    "model = Model(param_list[level])\n",
    "model.create_model(mode=NUMPY, check_transition=False, normalize_reward=False)\n",
    "model._model_to_numpy()\n",
    "\n",
    "partition = GenericPartition(model, discount, mode)\n",
    "\n",
    "value = np.zeros((model.state_dim))\n",
    "\n",
    "results = []\n",
    "\n",
    "n = 20\n",
    "\n",
    "for i in range(n):\n",
    "    value = optimal_bellman_operator(model, value, discount)\n",
    "    results.append(len(set((value))))\n",
    "\n",
    "plt.plot(results)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from utils.generic_model import NUMPY, SPARSE\n",
    "from utils.exact_value_function import get_exact_value, distance_to_optimal\n",
    "from utils.partition_value import ValuePartition\n",
    "from utils.partition_q_value import QValuePartition\n",
    "from utils.calculus_projected import apply_pobo_until_var_small, projected_optimal_bellman_operator, projected_optimal_q_bellman_operator\n",
    "from utils.calculus import norminf, get_value_policy_value, optimal_bellman_operator, q_optimal_bellman_operator, iterative_policy_evaluation, compute_transition_reward_policy, full_iterative_policy_evaluation, bellman_no_max\n",
    "from utils.partition_generic import GenericPartition\n",
    "import seaborn as sns\n",
    "from models.mountain import Model, param_list\n",
    "from tqdm import trange\n",
    "\n",
    "mode = SPARSE\n",
    "\n",
    "partition = GenericPartition(model, discount, mode)\n",
    "\n",
    "discount = 0.999\n",
    "epsilon = 1e-2\n",
    "level = 0\n",
    "model = Model(param_list[level])\n",
    "model.create_model(mode=NUMPY, check_transition=False, normalize_reward=False)\n",
    "model._model_to_numpy()\n",
    "\n",
    "print(model.state_dim)\n",
    "\n",
    "value = np.zeros((model.state_dim))\n",
    "\n",
    "results = []\n",
    "\n",
    "n = 20\n",
    "\n",
    "for i in range(n):\n",
    "    value = optimal_bellman_operator(model, value, discount)\n",
    "    results.append(len(set((value))))\n",
    "\n",
    "plt.plot(results)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from utils.generic_model import NUMPY, SPARSE\n",
    "from utils.exact_value_function import get_exact_value, distance_to_optimal\n",
    "from utils.partition_value import ValuePartition\n",
    "from utils.partition_q_value import QValuePartition\n",
    "from utils.calculus_projected import apply_pobo_until_var_small, projected_optimal_bellman_operator, projected_optimal_q_bellman_operator\n",
    "from utils.calculus import norminf, get_value_policy_value, optimal_bellman_operator, q_optimal_bellman_operator, iterative_policy_evaluation, compute_transition_reward_policy, full_iterative_policy_evaluation, bellman_no_max\n",
    "from utils.partition_generic import GenericPartition\n",
    "import seaborn as sns\n",
    "from models.rooms import Model, param_list\n",
    "from tqdm import trange\n",
    "\n",
    "mode = SPARSE\n",
    "\n",
    "discount = 0.99\n",
    "epsilon = 1e-2\n",
    "level = 0\n",
    "model = Model(param_list[level])\n",
    "model.create_model(mode=NUMPY, check_transition=False, normalize_reward=False)\n",
    "model._model_to_numpy()\n",
    "\n",
    "partition = GenericPartition(model, discount, mode)\n",
    "\n",
    "print(model.state_dim)\n",
    "\n",
    "value = np.zeros((model.state_dim))\n",
    "\n",
    "results = []\n",
    "\n",
    "n = 50\n",
    "\n",
    "for i in range(n):\n",
    "    value = optimal_bellman_operator(model, value, discount)\n",
    "    results.append(len(set((value/epsilon).astype(int))))\n",
    "\n",
    "plt.plot(results)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from utils.generic_model import NUMPY, SPARSE\n",
    "from utils.exact_value_function import get_exact_value, distance_to_optimal\n",
    "from utils.partition_value import ValuePartition\n",
    "from utils.partition_q_value import QValuePartition\n",
    "from utils.calculus_projected import apply_pobo_until_var_small, projected_optimal_bellman_operator, projected_optimal_q_bellman_operator\n",
    "from utils.calculus import norminf, get_value_policy_value, optimal_bellman_operator, q_optimal_bellman_operator, iterative_policy_evaluation, compute_transition_reward_policy, full_iterative_policy_evaluation, bellman_no_max\n",
    "from utils.partition_generic import GenericPartition\n",
    "import seaborn as sns\n",
    "from models.tandem import Model, param_list\n",
    "from tqdm import trange\n",
    "\n",
    "mode = SPARSE\n",
    "\n",
    "discount = 0.999\n",
    "epsilon = 0.1\n",
    "level = 0\n",
    "model = Model(param_list[level])\n",
    "model.create_model(mode=NUMPY, check_transition=False, normalize_reward=False)\n",
    "model._model_to_numpy()\n",
    "\n",
    "partition = GenericPartition(model, discount, mode)\n",
    "\n",
    "print(model.state_dim)\n",
    "\n",
    "value = np.zeros((model.state_dim))\n",
    "\n",
    "results = []\n",
    "\n",
    "n = 50\n",
    "\n",
    "for i in range(n):\n",
    "    value = optimal_bellman_operator(model, value, discount)\n",
    "    results.append(len(set((value/epsilon).astype(int)))/model.state_dim)\n",
    "\n",
    "plt.plot(results)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from utils.generic_model import NUMPY, SPARSE\n",
    "from utils.exact_value_function import get_exact_value, distance_to_optimal\n",
    "from utils.partition_value import ValuePartition\n",
    "from utils.partition_q_value import QValuePartition\n",
    "from utils.calculus_projected import apply_pobo_until_var_small, projected_optimal_bellman_operator, projected_optimal_q_bellman_operator\n",
    "from utils.calculus import norminf, get_value_policy_value, optimal_bellman_operator, q_optimal_bellman_operator, iterative_policy_evaluation, compute_transition_reward_policy, full_iterative_policy_evaluation, bellman_no_max\n",
    "from utils.partition_generic import GenericPartition\n",
    "import seaborn as sns\n",
    "from models.rooms import Model, param_list\n",
    "from tqdm import trange\n",
    "\n",
    "mode = SPARSE\n",
    "\n",
    "discount = 0.999\n",
    "epsilon = 0.1\n",
    "epsiprime = 0.001\n",
    "level = 0\n",
    "model = Model(param_list[level])\n",
    "model.create_model(mode=NUMPY, check_transition=False, normalize_reward=False)\n",
    "model._model_to_numpy()\n",
    "\n",
    "model.reward_matrix += epsiprime*np.random.random((model.state_dim, model.action_dim))\n",
    "\n",
    "partition = GenericPartition(model, discount, mode)\n",
    "\n",
    "print(model.state_dim)\n",
    "\n",
    "value = np.zeros((model.state_dim))\n",
    "\n",
    "results = []\n",
    "\n",
    "n = 50\n",
    "\n",
    "for i in range(n):\n",
    "    value = optimal_bellman_operator(model, value, discount)\n",
    "    results.append(len(set((value/epsilon).astype(int)))/model.state_dim)\n",
    "\n",
    "plt.plot(results)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from utils.generic_model import NUMPY, SPARSE\n",
    "from utils.exact_value_function import get_exact_value, distance_to_optimal\n",
    "from utils.partition_value import ValuePartition\n",
    "from utils.partition_q_value import QValuePartition\n",
    "from utils.calculus_projected import apply_pobo_until_var_small, projected_optimal_bellman_operator, projected_optimal_q_bellman_operator\n",
    "from utils.calculus import norminf, get_value_policy_value, optimal_bellman_operator, q_optimal_bellman_operator, iterative_policy_evaluation, compute_transition_reward_policy, full_iterative_policy_evaluation, bellman_no_max\n",
    "from utils.partition_generic import GenericPartition\n",
    "import seaborn as sns\n",
    "from models.taxi import Model\n",
    "from tqdm import trange\n",
    "\n",
    "discount = 0.999\n",
    "epsilon = 1e-2\n",
    "level = 0\n",
    "model = Model(100, 4)\n",
    "model.create_model(mode=NUMPY, check_transition=False, normalize_reward=False)\n",
    "model._model_to_numpy()\n",
    "\n",
    "value = np.zeros((model.state_dim))\n",
    "\n",
    "results = []\n",
    "\n",
    "n = 20\n",
    "\n",
    "for i in range(n):\n",
    "    value = optimal_bellman_operator(model, value, discount)\n",
    "    results.append(len(set((value))))\n",
    "\n",
    "plt.plot(results)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bellman_update_variation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Value Iteration base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dis\n",
    "from gurobipy import norm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from utils.generic_model import NUMPY, SPARSE\n",
    "from utils.exact_value_function import get_exact_value, distance_to_optimal\n",
    "from utils.partition_value import ValuePartition\n",
    "from utils.partition_q_value import QValuePartition\n",
    "from utils.calculus_projected import apply_pobo_until_var_small, projected_optimal_bellman_operator, projected_optimal_q_bellman_operator\n",
    "from utils.calculus import norminf, get_value_policy_value, optimal_bellman_operator, q_optimal_bellman_operator, iterative_policy_evaluation, compute_transition_reward_policy, full_iterative_policy_evaluation, bellman_no_max, optimal_bellman_residual\n",
    "from utils.partition_generic import GenericPartition\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm import trange\n",
    "from solvers.personal_vi import Solver\n",
    "\n",
    "from models.management import Model\n",
    "\n",
    "discount = 0.999\n",
    "epsilon = 1e-2\n",
    "model = Model(5000, 32)\n",
    "model.create_model(mode=SPARSE, check_transition=False, normalize_reward=True)\n",
    "model._model_to_sparse()\n",
    "\n",
    "\n",
    "sol = Solver(model, discount, epsilon)\n",
    "sol.run()\n",
    "print(np.round(sol.runtime, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### bellmanupdate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from utils.generic_model import NUMPY, SPARSE\n",
    "from utils.exact_value_function import get_exact_value, distance_to_optimal\n",
    "from utils.partition_value import ValuePartition\n",
    "from utils.partition_q_value import QValuePartition\n",
    "from utils.calculus_projected import apply_pobo_until_var_small, projected_optimal_bellman_operator, projected_optimal_q_bellman_operator\n",
    "from utils.calculus import norminf, get_value_policy_value, optimal_bellman_operator, q_optimal_bellman_operator, iterative_policy_evaluation, compute_transition_reward_policy, full_iterative_policy_evaluation, bellman_no_max\n",
    "from utils.partition_generic import GenericPartition\n",
    "import seaborn as sns\n",
    "from models.garnet import Model\n",
    "from tqdm import trange\n",
    "from solvers.aggregated_vi import Solver\n",
    "\n",
    "discount = 0.999\n",
    "epsilon = 1e-2\n",
    "level = 0\n",
    "model = Model(1000, 4)\n",
    "model.create_model(mode=SPARSE, check_transition=False, normalize_reward=False)\n",
    "model._model_to_sparse()\n",
    "\n",
    "sol = Solver(model, discount, 1e-2, bellman_updates=1)\n",
    "sol.run()\n",
    "print(np.round(sol.runtime, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### epsilon decreasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from utils.generic_model import NUMPY, SPARSE\n",
    "from utils.exact_value_function import get_exact_value, distance_to_optimal\n",
    "from utils.partition_value import ValuePartition\n",
    "from utils.partition_q_value import QValuePartition\n",
    "from utils.calculus_projected import apply_pobo_until_var_small, projected_optimal_bellman_operator, projected_optimal_q_bellman_operator\n",
    "from utils.calculus import norminf, get_value_policy_value, optimal_bellman_operator, q_optimal_bellman_operator, iterative_policy_evaluation, compute_transition_reward_policy, full_iterative_policy_evaluation, bellman_no_max\n",
    "from utils.partition_generic import GenericPartition\n",
    "import seaborn as sns\n",
    "from models.garnet import Model\n",
    "from tqdm import trange\n",
    "from solvers.aggregated_vi import Solver\n",
    "\n",
    "discount = 0.999\n",
    "epsilon = 1e-2\n",
    "level = 0\n",
    "model = Model(1000, 4)\n",
    "model.create_model(mode=SPARSE, check_transition=False, normalize_reward=False)\n",
    "model._model_to_sparse()\n",
    "\n",
    "sol = Solver(model, discount, 1e-2, bellman_updates=1, decreasing_span_bound=False)\n",
    "sol.run(max_agg_step=100)\n",
    "print(np.round(sol.runtime, 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Max agg steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dis\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from utils.generic_model import NUMPY, SPARSE\n",
    "from utils.exact_value_function import get_exact_value, distance_to_optimal\n",
    "from utils.partition_value import ValuePartition\n",
    "from utils.partition_q_value import QValuePartition\n",
    "from utils.calculus_projected import apply_pobo_until_var_small, projected_optimal_bellman_operator, projected_optimal_q_bellman_operator\n",
    "from utils.calculus import norminf, get_value_policy_value, optimal_bellman_operator, q_optimal_bellman_operator, iterative_policy_evaluation, compute_transition_reward_policy, full_iterative_policy_evaluation, bellman_no_max, optimal_bellman_residual\n",
    "from utils.partition_generic import GenericPartition\n",
    "import seaborn as sns\n",
    "from models.rooms import Model\n",
    "from tqdm import trange\n",
    "from solvers.aggregated_vi import Solver\n",
    "\n",
    "discount = 0.999\n",
    "epsilon = 1e-2\n",
    "model = Model(5000, 4)\n",
    "model.create_model(mode=SPARSE, check_transition=False, normalize_reward=False)\n",
    "model._model_to_sparse()\n",
    "\n",
    "sol = Solver(model, discount, 1e-2, bellman_updates=1)\n",
    "sol.run(max_agg_step=100)\n",
    "print(np.round(sol.runtime, 1))\n",
    "print(optimal_bellman_residual(model, sol.value, discount))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dis\n",
    "from gurobipy import norm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from utils.generic_model import NUMPY, SPARSE\n",
    "from utils.exact_value_function import get_exact_value, distance_to_optimal\n",
    "from utils.partition_value import ValuePartition\n",
    "from utils.partition_q_value import QValuePartition\n",
    "from utils.calculus_projected import apply_pobo_until_var_small, projected_optimal_bellman_operator, projected_optimal_q_bellman_operator\n",
    "from utils.calculus import norminf, get_value_policy_value, optimal_bellman_operator, q_optimal_bellman_operator, iterative_policy_evaluation, compute_transition_reward_policy, full_iterative_policy_evaluation, bellman_no_max, optimal_bellman_residual\n",
    "from utils.partition_generic import GenericPartition\n",
    "import seaborn as sns\n",
    "from models.management import Model\n",
    "from tqdm import trange\n",
    "from solvers.aggregated_vi import Solver\n",
    "\n",
    "discount = 0.999\n",
    "epsilon = 1e-2\n",
    "model = Model(5000, 32)\n",
    "model.create_model(mode=SPARSE, check_transition=False, normalize_reward=True)\n",
    "model._model_to_sparse()\n",
    "\n",
    "sol = Solver(model, discount, 1e-2, bellman_updates=1)\n",
    "\n",
    "sol.run(with_norm_weights=True, norm_method='average_reward')\n",
    "# sol.run()\n",
    "print(np.round(sol.runtime, 1))\n",
    "print(optimal_bellman_residual(model, sol.value, discount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dis\n",
    "from gurobipy import norm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from utils.generic_model import NUMPY, SPARSE\n",
    "from utils.exact_value_function import get_exact_value, distance_to_optimal\n",
    "from utils.partition_value import ValuePartition\n",
    "from utils.partition_q_value import QValuePartition\n",
    "from utils.calculus_projected import apply_pobo_until_var_small, projected_optimal_bellman_operator, projected_optimal_q_bellman_operator\n",
    "from utils.calculus import norminf, get_value_policy_value, optimal_bellman_operator, q_optimal_bellman_operator, iterative_policy_evaluation, compute_transition_reward_policy, full_iterative_policy_evaluation, bellman_no_max, optimal_bellman_residual\n",
    "from utils.partition_generic import GenericPartition\n",
    "import seaborn as sns\n",
    "from models.garnet import Model\n",
    "from tqdm import trange\n",
    "from solvers.aggregated_vi import Solver\n",
    "\n",
    "discount = 0.999\n",
    "epsilon = 1e-2\n",
    "model = Model(1000, 4)\n",
    "model.create_model(mode=SPARSE, check_transition=False, normalize_reward=True)\n",
    "model._model_to_sparse()\n",
    "\n",
    "sol = Solver(model, discount, 1e-2, bellman_updates=1)\n",
    "\n",
    "sol.run(with_norm_weights=True, norm_method='average_reward')\n",
    "# sol.run()\n",
    "print(np.round(sol.runtime, 1))\n",
    "print(optimal_bellman_residual(model, sol.value, discount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dis\n",
    "from gurobipy import norm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from utils.generic_model import NUMPY, SPARSE\n",
    "from utils.exact_value_function import get_exact_value, distance_to_optimal\n",
    "from utils.partition_value import ValuePartition\n",
    "from utils.partition_q_value import QValuePartition\n",
    "from utils.calculus_projected import apply_pobo_until_var_small, projected_optimal_bellman_operator, projected_optimal_q_bellman_operator\n",
    "from utils.calculus import norminf, get_value_policy_value, optimal_bellman_operator, q_optimal_bellman_operator, iterative_policy_evaluation, compute_transition_reward_policy, full_iterative_policy_evaluation, bellman_no_max, optimal_bellman_residual\n",
    "from utils.partition_generic import GenericPartition\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm import trange\n",
    "from solvers.aggregated_vi_options import Solver\n",
    "\n",
    "from models.garnet import Model\n",
    "\n",
    "discount = 0.999\n",
    "epsilon = 1e-2\n",
    "model = Model(1000, 4)\n",
    "model.create_model(mode=SPARSE, check_transition=False, normalize_reward=True)\n",
    "model._model_to_sparse()\n",
    "\n",
    "\n",
    "sol = Solver(model, discount, epsilon)\n",
    "sol.run(with_norm_weights=True)\n",
    "print(np.round(sol.runtime, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oil problem exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dis\n",
    "from gurobipy import norm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from utils.generic_model import NUMPY, SPARSE\n",
    "from utils.exact_value_function import get_exact_value, distance_to_optimal\n",
    "from utils.partition_value import ValuePartition\n",
    "from utils.partition_q_value import QValuePartition\n",
    "from utils.calculus_projected import apply_pobo_until_var_small, projected_optimal_bellman_operator, projected_optimal_q_bellman_operator\n",
    "from utils.calculus import norminf, get_value_policy_value, optimal_bellman_operator, q_optimal_bellman_operator, iterative_policy_evaluation, compute_transition_reward_policy, full_iterative_policy_evaluation, bellman_no_max, optimal_bellman_residual\n",
    "from utils.partition_generic import GenericPartition\n",
    "import seaborn as sns\n",
    "from models.oil import Model\n",
    "from tqdm import trange\n",
    "from solvers.aggregated_vi import Solver\n",
    "\n",
    "discount = 0.9\n",
    "epsilon = 1e-2\n",
    "model = Model(200, 4)\n",
    "model.create_model(mode=SPARSE, check_transition=False, normalize_reward=True)\n",
    "model._model_to_sparse()\n",
    "\n",
    "val = get_exact_value(model, 'discounted', discount)\n",
    "plt.plot(val)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ambulance problem exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dis\n",
    "from gurobipy import norm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from utils.generic_model import NUMPY, SPARSE\n",
    "from utils.exact_value_function import get_exact_value, distance_to_optimal\n",
    "from utils.partition_value import ValuePartition\n",
    "from utils.partition_q_value import QValuePartition\n",
    "from utils.calculus_projected import apply_pobo_until_var_small, projected_optimal_bellman_operator, projected_optimal_q_bellman_operator\n",
    "from utils.calculus import norminf, get_value_policy_value, optimal_bellman_operator, q_optimal_bellman_operator, iterative_policy_evaluation, compute_transition_reward_policy, full_iterative_policy_evaluation, bellman_no_max, optimal_bellman_residual\n",
    "from utils.partition_generic import GenericPartition\n",
    "import seaborn as sns\n",
    "from models.ambulance import Model\n",
    "from tqdm import trange\n",
    "from solvers.aggregated_vi import Solver\n",
    "\n",
    "discount = 0.9\n",
    "epsilon = 1e-2\n",
    "model = Model(25, 4)\n",
    "model.create_model(mode=SPARSE, check_transition=True, normalize_reward=True)\n",
    "model._model_to_sparse()\n",
    "\n",
    "val = get_exact_value(model, 'discounted', discount)\n",
    "plt.plot(val)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dis\n",
    "from gurobipy import norm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from utils.generic_model import NUMPY, SPARSE\n",
    "from utils.exact_value_function import get_exact_value, distance_to_optimal\n",
    "from utils.partition_value import ValuePartition\n",
    "from utils.partition_q_value import QValuePartition\n",
    "from utils.calculus_projected import apply_pobo_until_var_small, projected_optimal_bellman_operator, projected_optimal_q_bellman_operator\n",
    "from utils.calculus import norminf, get_value_policy_value, optimal_bellman_operator, q_optimal_bellman_operator, iterative_policy_evaluation, compute_transition_reward_policy, full_iterative_policy_evaluation, bellman_no_max, optimal_bellman_residual\n",
    "from utils.partition_generic import GenericPartition\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm import trange\n",
    "from solvers.aggregated_vi_tiles import Solver\n",
    "\n",
    "from models.garnet import Model\n",
    "\n",
    "discount = 0.999\n",
    "epsilon = 1e-2\n",
    "model = Model(1000, 4)\n",
    "model.create_model(mode=SPARSE, check_transition=False, normalize_reward=True)\n",
    "model._model_to_sparse()\n",
    "\n",
    "\n",
    "sol = Solver(model, discount, epsilon, bellman_updates=1, n_tiles=15)\n",
    "sol.run()\n",
    "print(np.round(sol.runtime, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dis\n",
    "from gurobipy import norm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from utils.generic_model import NUMPY, SPARSE\n",
    "from utils.exact_value_function import get_exact_value, distance_to_optimal\n",
    "from utils.partition_value import ValuePartition\n",
    "from utils.partition_q_value import QValuePartition\n",
    "from utils.calculus_projected import apply_pobo_until_var_small, projected_optimal_bellman_operator, projected_optimal_q_bellman_operator\n",
    "from utils.calculus import norminf, get_value_policy_value, optimal_bellman_operator, q_optimal_bellman_operator, iterative_policy_evaluation, compute_transition_reward_policy, full_iterative_policy_evaluation, bellman_no_max, optimal_bellman_residual\n",
    "from utils.partition_generic import GenericPartition\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm import trange\n",
    "from solvers.aggregated_vi_tiles import Solver\n",
    "\n",
    "from models.management import Model\n",
    "\n",
    "discount = 0.999\n",
    "epsilon = 1e-2\n",
    "model = Model(5000, 32)\n",
    "model.create_model(mode=SPARSE, check_transition=False, normalize_reward=True)\n",
    "model._model_to_sparse()\n",
    "\n",
    "\n",
    "sol = Solver(model, discount, epsilon, bellman_updates=1, n_tiles=rc)\n",
    "sol.run()\n",
    "print(np.round(sol.runtime, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dis\n",
    "from gurobipy import norm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from utils.generic_model import NUMPY, SPARSE\n",
    "from utils.exact_value_function import get_exact_value, distance_to_optimal\n",
    "from utils.partition_value import ValuePartition\n",
    "from utils.partition_q_value import QValuePartition\n",
    "from utils.calculus_projected import apply_pobo_until_var_small, projected_optimal_bellman_operator, projected_optimal_q_bellman_operator\n",
    "from utils.calculus import norminf, get_value_policy_value, optimal_bellman_operator, q_optimal_bellman_operator, iterative_policy_evaluation, compute_transition_reward_policy, full_iterative_policy_evaluation, bellman_no_max, optimal_bellman_residual\n",
    "from utils.partition_generic import GenericPartition\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm import trange\n",
    "from solvers.personal_vi import Solver\n",
    "\n",
    "from models.management import Model\n",
    "\n",
    "discount = 0.999\n",
    "epsilon = 1e-2\n",
    "model = Model(5000, 32)\n",
    "model.create_model(mode=SPARSE, check_transition=False, normalize_reward=True)\n",
    "model._model_to_sparse()\n",
    "\n",
    "\n",
    "sol = Solver(model, discount, epsilon)\n",
    "sol.run()\n",
    "print(np.round(sol.runtime, 1))\n",
    "plt.plot(sol.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MDP equivalents ou pas ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from utils.generic_model import NUMPY, SPARSE\n",
    "from utils.exact_value_function import get_exact_value, distance_to_optimal\n",
    "from utils.partition_value import ValuePartition\n",
    "from utils.partition_q_value import QValuePartition\n",
    "from utils.calculus_projected import apply_pobo_until_var_small, projected_optimal_bellman_operator, projected_optimal_q_bellman_operator\n",
    "from utils.calculus import norminf, get_value_policy_value, optimal_bellman_operator, q_optimal_bellman_operator, iterative_policy_evaluation, compute_transition_reward_policy, full_iterative_policy_evaluation, bellman_no_max\n",
    "from utils.partition_generic import GenericPartition\n",
    "import seaborn as sns\n",
    "from models.rooms import Model\n",
    "from tqdm import trange\n",
    "from solvers.aggregated_vi import Solver\n",
    "\n",
    "discount = 0.999\n",
    "epsilon = 1e-2\n",
    "level = 0\n",
    "model = Model(100, 4)\n",
    "model.create_model(mode=SPARSE, check_transition=False, normalize_reward=False)\n",
    "model._model_to_sparse()\n",
    "\n",
    "sol = Solver(model, discount, epsilon)\n",
    "sol.run()\n",
    "sol.partition._number_of_regions()\n",
    "\n",
    "policy = np.argmax(bellman_no_max(model, get_exact_value(model, 'discounted', discount), discount), axis=1)\n",
    "value = np.random.randint(2, size=model.state_dim)\n",
    "\n",
    "phi = sol.partition._partial_phi()\n",
    "omega = sol.partition.weights\n",
    "averaging_matrix = phi.dot(omega)\n",
    "\n",
    "full_state_transition = [averaging_matrix.dot(transition.dot(averaging_matrix)) for transition in model.transition_matrix]\n",
    "full_state_reward = averaging_matrix.dot(model.reward_matrix)\n",
    "\n",
    "from utils.generic_model import CustomModel\n",
    "\n",
    "averaged_model = CustomModel(full_state_transition, full_state_reward, 'custom')\n",
    "\n",
    "from utils.calculus import bellman_policy_operator\n",
    "\n",
    "transition_full_policy, reward_full_policy = compute_transition_reward_policy(averaged_model, policy)\n",
    "bellman_policy_operator(value, discount, transition_full_policy, reward_full_policy)\n",
    "\n",
    "transition_policy, reward_policy = compute_transition_reward_policy(model, policy)\n",
    "bellman_policy_operator(value, discount, transition_policy, reward_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectrum transition matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_spectrum(matrix):\n",
    "    eigenvalues = np.linalg.eigvals(matrix)\n",
    "    plt.scatter(eigenvalues.real, eigenvalues.imag, color='blue', marker='o')\n",
    "    plt.axhline(0, color='black',linewidth=0.5)\n",
    "    plt.axvline(0, color='black',linewidth=0.5)\n",
    "    plt.grid(color = 'gray', linestyle = '--', linewidth = 0.5)\n",
    "    plt.xlabel('Real Part')\n",
    "    plt.ylabel('Imaginary Part')\n",
    "    plt.title('Spectrum of the Matrix')\n",
    "    plt.show()\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from utils.generic_model import NUMPY, SPARSE\n",
    "from utils.exact_value_function import get_exact_value, distance_to_optimal\n",
    "from utils.partition_value import ValuePartition\n",
    "from utils.partition_q_value import QValuePartition\n",
    "from utils.calculus_projected import apply_pobo_until_var_small, projected_optimal_bellman_operator, projected_optimal_q_bellman_operator\n",
    "from utils.calculus import norminf, get_value_policy_value, optimal_bellman_operator, q_optimal_bellman_operator, iterative_policy_evaluation, compute_transition_reward_policy, full_iterative_policy_evaluation, bellman_no_max\n",
    "from utils.partition_generic import GenericPartition\n",
    "import seaborn as sns\n",
    "from models.rooms import Model\n",
    "from tqdm import trange\n",
    "from solvers.aggregated_vi import Solver\n",
    "\n",
    "discount = 0.999\n",
    "epsilon = 1e-2\n",
    "level = 0\n",
    "model = Model(100, 4)\n",
    "model.create_model(mode=SPARSE, check_transition=False, normalize_reward=False)\n",
    "model._model_to_numpy()\n",
    "\n",
    "P = model.transition_matrix[0]\n",
    "R = model.reward_matrix[:, 0]\n",
    "\n",
    "plot_spectrum(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# data\n",
    "x = [4900, 10000, 49284, 99856, 250000, 1000000]\n",
    "y1 = [91.6, 173.6, 1167.7, 3419.5, 15858.2, 219789.2]\n",
    "y2 = [10.1, 11.1, 28.6, 61.9, 207.4, 2274.4]\n",
    "y3 = [77.6, 257.6, 8727.4, 46546.7, 437824.1, np.nan]\n",
    "y4 = [56.3, 111.2, 565.1, 2801.6, 4111.8, np.nan]\n",
    "y5 = [364.5, 987.0, 19748.5, 92574.3, 728272.9, np.nan]\n",
    "y6 = [1489.2, 2154.8, 5021.9, 7661.8, 14057.0, np.nan]\n",
    "y7 = [599.0, np.nan, 27137.2, 116716.5, 922369.5, np.nan]\n",
    "\n",
    "# create the plot\n",
    "plt.plot(x,y1,'-o', label='PDVI')\n",
    "plt.plot(x,y2,'-o', label='PDQVI')\n",
    "plt.plot(x,y3,'-o', label='PDPI')\n",
    "plt.plot(x,y4,'-o', label='VI')\n",
    "plt.plot(x,y5,'-o', label='PIM')\n",
    "plt.plot(x,y6,'-o', label='Chen')\n",
    "plt.plot(x,y7,'-o', label='Bertsekas')\n",
    "\n",
    "# set labels and title\n",
    "plt.xlabel('State dimension')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Plot of Table Data')\n",
    "plt.legend()\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simplification Impatience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.impatience import *\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "state = 200\n",
    "\n",
    "arrival_poisson_rate, max_queue_length, proba_person_stay = 0.9, state-1,  0.9\n",
    "\n",
    "ss1, aa, ss2 = 50, 10, 10\n",
    "\n",
    "f = lambda ss2 : transition_function(ss1, aa, ss2, arrival_poisson_rate, max_queue_length, proba_person_stay)\n",
    "\n",
    "values = [f(ss2) * float(abs(max(0, ss1 - aa - 1) - ss2) <= 0.08 * max_queue_length) for ss2 in range(state)]\n",
    "# plt.plot(values)\n",
    "# plt.yscale('log')\n",
    "\n",
    "# print(sum(values))\n",
    "# print(np.argmax(values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = [probability_surv(50, surv, 0.9)*float(abs(surv-50) < 0.2 * 100) for surv in range(100)]\n",
    "plt.plot(values)\n",
    "print(sum(values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = [probability_arrival(arr, 20, 0.9, 100) for arr in range(100-20)]\n",
    "plt.plot(values)\n",
    "print(sum(values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot value Mountain Car"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from utils.exact_value_function import get_exact_value, distance_to_optimal\n",
    "from utils.partition_value import ValuePartition\n",
    "from utils.calculus_projected import apply_pobo_until_var_small, projected_optimal_bellman_operator\n",
    "from utils.calculus import norminf, get_value_policy_value, optimal_bellman_operator\n",
    "from utils.partition_generic import GenericPartition\n",
    "from utils.data_management import import_models_from_file\n",
    "from utils.generic_model import GenericModel\n",
    "import seaborn as sns\n",
    "\n",
    "discount = 0.99\n",
    "epsilon = 1e-2\n",
    "Model = import_models_from_file('mountain')\n",
    "\n",
    "s = 100\n",
    "state = s**2\n",
    "\n",
    "model:GenericModel = Model(state, 10)\n",
    "model.create_model(False, False)\n",
    "exact_value = get_exact_value(model, 'discounted', discount)\n",
    "\n",
    "contracted_value = np.zeros((1))\n",
    "partition = GenericPartition(model, discount)\n",
    "\n",
    "partition._compute_weights_phi()\n",
    "partition.compute_agg_trans_reward_v()\n",
    "\n",
    "for i in range(3):\n",
    "    for _ in range(8):\n",
    "\n",
    "        for _ in range(10):\n",
    "            contracted_value = projected_optimal_bellman_operator(model, discount, contracted_value, partition.aggregate_transition_matrix, partition.aggregate_reward_matrix, partition.weights)\n",
    "\n",
    "        value = partition._partial_phi() @ contracted_value\n",
    "\n",
    "        bellman_value = optimal_bellman_operator(model, value, discount)\n",
    "\n",
    "        for _ in range(5):\n",
    "            bellman_value = optimal_bellman_operator(model, bellman_value, discount)\n",
    "\n",
    "        contracted_value = partition.divide_all_regions_along_value_update_contracted_value(bellman_value, 0.001, contracted_value)\n",
    "        partition._compute_weights_phi()\n",
    "        partition.compute_agg_trans_reward_v()\n",
    "\n",
    "    value = -(partition._partial_phi() @ contracted_value).reshape((s, s))\n",
    "    sns.heatmap(value, xticklabels=False, yticklabels=False, cbar=False, square=True, cmap=sns.color_palette(\"viridis\", as_cmap=True))\n",
    "    plt.savefig(\"mountain_abstraction_{}.png\".format(i), dpi=400)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "value = -(partition._partial_phi() @ contracted_value).reshape((s, s))\n",
    "sns.heatmap(value, xticklabels=False, yticklabels=False, square=True, cmap=sns.color_palette(\"viridis\", as_cmap=True))\n",
    "plt.savefig(\"mountain_abstraction_{}.png\".format(i), dpi=400)\n",
    "plt.show()\n",
    "\n",
    "print(partition._number_of_regions())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
